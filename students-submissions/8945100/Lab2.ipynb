{"cells":[{"cell_type":"markdown","metadata":{"id":"J0asGDFkQgv7"},"source":["## Lab Assignment 2\n","## Name: Nihal Patel\n","## Student ID: 8945100"]},{"cell_type":"markdown","metadata":{"id":"S-L6juuMQqKj"},"source":["Use the Lab2_dataset.csv provided.\n","## Preprocessing"]},{"cell_type":"code","execution_count":103,"metadata":{"executionInfo":{"elapsed":147,"status":"ok","timestamp":1696719484638,"user":{"displayName":"Nihal Patel","userId":"14123324452187374675"},"user_tz":240},"id":"Jex1IHgOMntA"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer"]},{"cell_type":"code","execution_count":104,"metadata":{"executionInfo":{"elapsed":147,"status":"ok","timestamp":1696719614026,"user":{"displayName":"Nihal Patel","userId":"14123324452187374675"},"user_tz":240},"id":"pojvQFyVMtGH"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>label</th>\n","      <th>text</th>\n","      <th>label_num</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>605</td>\n","      <td>ham</td>\n","      <td>Subject: enron methanol ; meter # : 988291\\nth...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2349</td>\n","      <td>ham</td>\n","      <td>Subject: hpl nom for january 9 , 2001\\n( see a...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3624</td>\n","      <td>ham</td>\n","      <td>Subject: neon retreat\\nho ho ho , we ' re arou...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4685</td>\n","      <td>spam</td>\n","      <td>Subject: photoshop , windows , office . cheap ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2030</td>\n","      <td>ham</td>\n","      <td>Subject: re : indian springs\\nthis deal is to ...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0 label                                               text   \n","0         605   ham  Subject: enron methanol ; meter # : 988291\\nth...  \\\n","1        2349   ham  Subject: hpl nom for january 9 , 2001\\n( see a...   \n","2        3624   ham  Subject: neon retreat\\nho ho ho , we ' re arou...   \n","3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n","4        2030   ham  Subject: re : indian springs\\nthis deal is to ...   \n","\n","   label_num  \n","0          0  \n","1          0  \n","2          0  \n","3          1  \n","4          0  "]},"execution_count":104,"metadata":{},"output_type":"execute_result"}],"source":["# Load the data set\n","dataLab2 = pd.read_csv(\"C:/Users/nihal/Fall2023/students-submissions/8945100/Lab2_dataset.csv\")\n","dataLab2.head()"]},{"cell_type":"code","execution_count":105,"metadata":{"executionInfo":{"elapsed":140,"status":"ok","timestamp":1696719617779,"user":{"displayName":"Nihal Patel","userId":"14123324452187374675"},"user_tz":240},"id":"km-u8m6kPX5k"},"outputs":[],"source":["# We will use Countervectorizer for the transformation of the text feature to vector representation and split the data into features (X) and labels (y)\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(dataLab2['text'])\n","y = dataLab2[\"label_num\"]"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[],"source":["# We will now split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=20)"]},{"cell_type":"markdown","metadata":{},"source":["## Part A\n","\n","## Model Training and Evaluation"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[],"source":["# According to the above process of Preprocessing, now we have X_train, X_test, y_train and y_test for further analysis\n","# We will import the \n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, classification_report"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[],"source":["# Further we will Initialize the models which we have imported above\n","svcModel = SVC()\n","gnbModel = GaussianNB()\n","multinomialnbModel = MultinomialNB()"]},{"cell_type":"code","execution_count":109,"metadata":{},"outputs":[{"data":{"text/html":["<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"],"text/plain":["MultinomialNB()"]},"execution_count":109,"metadata":{},"output_type":"execute_result"}],"source":["# Now, we will train the models on the training data\n","svcModel.fit(X_train, y_train)\n","gnbModel.fit(X_train.toarray(), y_train)\n","multinomialnbModel.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":110,"metadata":{},"outputs":[],"source":["# We will predict the labels for the test set\n","svcPrediction = svcModel.predict(X_test)\n","gnbPrediction = gnbModel.predict(X_test.toarray())\n","multinomialnbPrediction = multinomialnbModel.predict(X_test)"]},{"cell_type":"code","execution_count":111,"metadata":{},"outputs":[],"source":["# We will evaluate the models\n","svcAccuracy = accuracy_score(y_test, svcPrediction)\n","gnbAccuracy = accuracy_score(y_test, gnbPrediction)\n","multinomialnbAccuracy = accuracy_score(y_test, multinomialnbPrediction)"]},{"cell_type":"code","execution_count":112,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["SVC Accuracy:\n"," 0.961352657004831\n","SVC Classification Report:\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.96      0.97       744\n","           1       0.91      0.96      0.93       291\n","\n","    accuracy                           0.96      1035\n","   macro avg       0.95      0.96      0.95      1035\n","weighted avg       0.96      0.96      0.96      1035\n","\n","Gaussian Naive Bayes Accuracy:\n"," 0.9536231884057971\n","Gaussian Naive Bayes Classification Report:\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.95      0.98      0.97       744\n","           1       0.96      0.88      0.91       291\n","\n","    accuracy                           0.95      1035\n","   macro avg       0.95      0.93      0.94      1035\n","weighted avg       0.95      0.95      0.95      1035\n","\n","Multinomial Naive Bayes Accuracy:\n"," 0.9797101449275363\n","Multinomial Naive Bayes Classification Report:\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.99      0.99       744\n","           1       0.96      0.97      0.96       291\n","\n","    accuracy                           0.98      1035\n","   macro avg       0.97      0.98      0.97      1035\n","weighted avg       0.98      0.98      0.98      1035\n","\n"]}],"source":["# Now We will print the accuracy and classification reports for each model\n","print(\"SVC Accuracy:\\n\", svcAccuracy)\n","print(\"SVC Classification Report:\\n\")\n","print(classification_report(y_test, svcPrediction))\n","\n","print(\"Gaussian Naive Bayes Accuracy:\\n\", gnbAccuracy)\n","print(\"Gaussian Naive Bayes Classification Report:\\n\")\n","print(classification_report(y_test, gnbPrediction))\n","\n","\n","print(\"Multinomial Naive Bayes Accuracy:\\n\", multinomialnbAccuracy)\n","print(\"Multinomial Naive Bayes Classification Report:\\n\")\n","print(classification_report(y_test, multinomialnbPrediction))"]},{"cell_type":"markdown","metadata":{},"source":["Based on the above results, we will compare the performance of the SVC, Gaussian Naive Bayes and Multinomial Naive Bayes models. There are several factors on which we can find the difference between the 3 models:\n","1. Assumption of Models: \n","SVC : This model is not making any assumptions like that of Naive Bayes Model. There can be a possibility of complex relationships between models considering the independent assumptions.\n","Gaussian Naive Bayes Model: The assumption for this model can be a suboptimal performance as it follows a Normal distribution.\n","Multinomial Naive Bayes Model: This model is suitable for text data and it does not target to assume any single feature of distribution.\n","\n","2. Distribution of Data:\n","Involvement of categorical features like text data which includes word count, word presence / absence is displayed in a better way in Multinomial Naive Bayes. The Gausian Naive Bayes has a continuos distribution assumption which has less chances of aligning with the nature of text data.\n","\n","3. Differences between the accuracy:\n","The SVC model is seen performing well but not like that of the Multionomial Model. The complexity of model and hyperparameter tuning might be the reasons behind the performance.\n","The Multinomial Model shows the highest accuracy due to the appropriate task of text classification and the nature of dataset.\n","The Gausian Naive Bayes model has the lowest accuracy due to its assumption which is not aligning with text data.\n","\n","In order to conclude, choosing the most suitable model depends on the nature of of the data and specific task. The Multinomial Naive Bayes is a good option for text classification as it does not rely its assumption on a specific distribution and can handle word count based on its frequency. The hyperparameter for SVC model may improve its performace considering the complexity in the relation between features and labels.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Part B\n","\n","Use the AB_NYC_2019.csv dataset for this part."]},{"cell_type":"code","execution_count":113,"metadata":{},"outputs":[],"source":["import numpy as np\n","from scipy import stats"]},{"cell_type":"code","execution_count":114,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>name</th>\n","      <th>host_id</th>\n","      <th>host_name</th>\n","      <th>neighbourhood_group</th>\n","      <th>neighbourhood</th>\n","      <th>latitude</th>\n","      <th>longitude</th>\n","      <th>room_type</th>\n","      <th>price</th>\n","      <th>minimum_nights</th>\n","      <th>number_of_reviews</th>\n","      <th>last_review</th>\n","      <th>reviews_per_month</th>\n","      <th>calculated_host_listings_count</th>\n","      <th>availability_365</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2539</td>\n","      <td>Clean &amp; quiet apt home by the park</td>\n","      <td>2787</td>\n","      <td>John</td>\n","      <td>Brooklyn</td>\n","      <td>Kensington</td>\n","      <td>40.64749</td>\n","      <td>-73.97237</td>\n","      <td>Private room</td>\n","      <td>149</td>\n","      <td>1</td>\n","      <td>9</td>\n","      <td>2018-10-19</td>\n","      <td>0.21</td>\n","      <td>6</td>\n","      <td>365</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2595</td>\n","      <td>Skylit Midtown Castle</td>\n","      <td>2845</td>\n","      <td>Jennifer</td>\n","      <td>Manhattan</td>\n","      <td>Midtown</td>\n","      <td>40.75362</td>\n","      <td>-73.98377</td>\n","      <td>Entire home/apt</td>\n","      <td>225</td>\n","      <td>1</td>\n","      <td>45</td>\n","      <td>2019-05-21</td>\n","      <td>0.38</td>\n","      <td>2</td>\n","      <td>355</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3647</td>\n","      <td>THE VILLAGE OF HARLEM....NEW YORK !</td>\n","      <td>4632</td>\n","      <td>Elisabeth</td>\n","      <td>Manhattan</td>\n","      <td>Harlem</td>\n","      <td>40.80902</td>\n","      <td>-73.94190</td>\n","      <td>Private room</td>\n","      <td>150</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>365</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3831</td>\n","      <td>Cozy Entire Floor of Brownstone</td>\n","      <td>4869</td>\n","      <td>LisaRoxanne</td>\n","      <td>Brooklyn</td>\n","      <td>Clinton Hill</td>\n","      <td>40.68514</td>\n","      <td>-73.95976</td>\n","      <td>Entire home/apt</td>\n","      <td>89</td>\n","      <td>1</td>\n","      <td>270</td>\n","      <td>2019-07-05</td>\n","      <td>4.64</td>\n","      <td>1</td>\n","      <td>194</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5022</td>\n","      <td>Entire Apt: Spacious Studio/Loft by central park</td>\n","      <td>7192</td>\n","      <td>Laura</td>\n","      <td>Manhattan</td>\n","      <td>East Harlem</td>\n","      <td>40.79851</td>\n","      <td>-73.94399</td>\n","      <td>Entire home/apt</td>\n","      <td>80</td>\n","      <td>10</td>\n","      <td>9</td>\n","      <td>2018-11-19</td>\n","      <td>0.10</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     id                                              name  host_id   \n","0  2539                Clean & quiet apt home by the park     2787  \\\n","1  2595                             Skylit Midtown Castle     2845   \n","2  3647               THE VILLAGE OF HARLEM....NEW YORK !     4632   \n","3  3831                   Cozy Entire Floor of Brownstone     4869   \n","4  5022  Entire Apt: Spacious Studio/Loft by central park     7192   \n","\n","     host_name neighbourhood_group neighbourhood  latitude  longitude   \n","0         John            Brooklyn    Kensington  40.64749  -73.97237  \\\n","1     Jennifer           Manhattan       Midtown  40.75362  -73.98377   \n","2    Elisabeth           Manhattan        Harlem  40.80902  -73.94190   \n","3  LisaRoxanne            Brooklyn  Clinton Hill  40.68514  -73.95976   \n","4        Laura           Manhattan   East Harlem  40.79851  -73.94399   \n","\n","         room_type  price  minimum_nights  number_of_reviews last_review   \n","0     Private room    149               1                  9  2018-10-19  \\\n","1  Entire home/apt    225               1                 45  2019-05-21   \n","2     Private room    150               3                  0         NaN   \n","3  Entire home/apt     89               1                270  2019-07-05   \n","4  Entire home/apt     80              10                  9  2018-11-19   \n","\n","   reviews_per_month  calculated_host_listings_count  availability_365  \n","0               0.21                               6               365  \n","1               0.38                               2               355  \n","2                NaN                               1               365  \n","3               4.64                               1               194  \n","4               0.10                               1                 0  "]},"execution_count":114,"metadata":{},"output_type":"execute_result"}],"source":["# Load the data set\n","dataNYC = pd.read_csv(\"C:/Users/nihal/Fall2023/students-submissions/8945100/AB_NYC_2019.csv\")\n","dataNYC.head()"]},{"cell_type":"code","execution_count":115,"metadata":{},"outputs":[],"source":["# We will define a function to remove outliers using the Z-score approach\n","def remove_outliers_zscore(dataNYC, column, z_threshold=3):\n","    z_scores = np.abs(stats.zscore(dataNYC[column]))\n","    dataNYC_no_outliers = dataNYC[(z_scores < z_threshold)]\n","    return dataNYC_no_outliers"]},{"cell_type":"code","execution_count":116,"metadata":{},"outputs":[],"source":["# We will define a function to remove outliers using the whisker approach\n","def remove_outliers_zscore(dataNYC, threshold=3):\n","    z_scores = np.abs((dataNYC - dataNYC.mean()) / dataNYC.std())\n","    return dataNYC[z_scores < threshold]\n","\n","# We will define a function to remove outliers using whiskers approach\n","def remove_outliers_whiskers(dataNYC):\n","    Q1 = dataNYC.quantile(0.25)\n","    Q3 = dataNYC.quantile(0.75)\n","    IQR = Q3 - Q1\n","    lower_whisker = Q1 - 1.5 * IQR\n","    upper_whisker = Q3 + 1.5 * IQR\n","    return dataNYC[(dataNYC >= lower_whisker) & (dataNYC <= upper_whisker)]"]},{"cell_type":"code","execution_count":117,"metadata":{},"outputs":[],"source":["# We will apply the Z-score approach\n","clean_NYCdata_zscore = dataNYC.copy()\n","clean_NYCdata_zscore['price'] = remove_outliers_zscore(clean_NYCdata_zscore['price'])"]},{"cell_type":"code","execution_count":118,"metadata":{},"outputs":[],"source":["# We will apply the whiskers approach\n","clean_NYCdata_whiskers = dataNYC.copy()\n","clean_NYCdata_whiskers['price'] = remove_outliers_whiskers(clean_NYCdata_whiskers['price'])"]},{"cell_type":"code","execution_count":119,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Statistics after Z-score approach:\n"," count    48507.000000\n","mean       138.746903\n","std        107.558233\n","min          0.000000\n","25%         69.000000\n","50%        105.000000\n","75%        175.000000\n","max        860.000000\n","Name: price, dtype: float64\n","\n","\n","Statistics after whiskers approach:\n"," count    45923.000000\n","mean       119.970320\n","std         68.150148\n","min          0.000000\n","25%         65.000000\n","50%        100.000000\n","75%        159.000000\n","max        334.000000\n","Name: price, dtype: float64\n"]}],"source":["# Comparing the statistics of 'price' before and after removing outliers\n","print(\"Statistics after Z-score approach:\\n\",clean_NYCdata_zscore['price'].describe())\n","print(\"\\n\")\n","print(\"Statistics after whiskers approach:\\n\",clean_NYCdata_whiskers['price'].describe())"]},{"cell_type":"markdown","metadata":{},"source":["Based on the above results for both the Statistics, we can analyze as below:\n","1. The removal of outliers is resulted into decrease of mean price for both the scenarios as outliers have the capability to tend to skew the mean towards extreme values\n","2. As a result of outliers removal, there is a decrease in the standard deviation which means that the spread of data is more concentrated around the mean\n","3. There is a stability seen within the values of percentile which indicates that there is no change in the central tendency of data.\n","4. We can also notice that the maximum value is decreased after removal of outliers which resulted in indentification of extreme high values. "]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNPBDpYv8BE95xM3UTk/NlN","provenance":[]},"kernelspec":{"display_name":"CSCN8010_classic_ml","language":"python","name":"cscn8010_classic_ml"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
